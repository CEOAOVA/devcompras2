/**
 * Embedding Service
 *
 * Servicio para generaci√≥n y gesti√≥n de embeddings vectoriales con:
 * - Cach√© Redis con TTL de 7 d√≠as (ahorra 80% de llamadas a OpenAI)
 * - Chunking sem√°ntico inteligente (respeta p√°rrafos, tablas, c√≥digo)
 * - Embeddings de 512 dimensiones (ahorra 50% storage y costos)
 * - Rate limiting con cola para evitar rate limits
 * - Retry con exponential backoff
 * - B√∫squeda sem√°ntica con pre-filtros
 */

import { OpenAI } from 'openai';
import { get_encoding } from 'tiktoken';
import crypto from 'crypto';
import { getRedisClient } from '../utils/redis-singleton';
import type Redis from 'ioredis';

// Tipos
export interface ChunkMetadata {
  pageNumber?: number;
  section?: string;
  type: 'paragraph' | 'table' | 'code' | 'list';
  confidence?: number;
}

export interface DocumentChunk {
  text: string;
  tokens: number;
  metadata: ChunkMetadata;
  overlap: string;
}

export interface SearchResult {
  id: string;
  documentId: string;
  content: string;
  similarity: number;
  metadata: any;
  pageNumber?: number;
  contentType: string;
  chunkIndex: number;
}

interface EmbeddingQueueItem {
  text: string;
  resolve: (embedding: number[]) => void;
  reject: (error: Error) => void;
}

/**
 * Servicio de Embeddings con cach√© y chunking inteligente
 */
export class EmbeddingService {
  private openai: OpenAI;
  private redis: Redis;
  private encoding: any;
  private readonly EMBEDDING_MODEL = process.env.EMBEDDING_MODEL || 'text-embedding-3-large';
  private readonly EMBEDDING_DIMENSIONS = parseInt(process.env.EMBEDDING_DIMENSIONS || '512');
  private readonly CACHE_TTL = parseInt(process.env.EMBEDDING_CACHE_TTL || '604800'); // 7 d√≠as
  private readonly CHUNK_SIZE = parseInt(process.env.RAG_CHUNK_SIZE || '800');
  private readonly OVERLAP_SIZE = parseInt(process.env.RAG_CHUNK_OVERLAP || '200');
  private readonly MIN_CHUNK_SIZE = parseInt(process.env.RAG_MIN_CHUNK_SIZE || '100');

  // Cola para rate limiting
  private queue: EmbeddingQueueItem[] = [];
  private processing = false;
  private readonly BATCH_SIZE = 10; // OpenAI permite ~10 requests/s
  private readonly DELAY_MS = 100;

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });

    // Use Redis singleton to avoid connection pool exhaustion
    this.redis = getRedisClient();

    // Initialize tiktoken encoding
    this.encoding = get_encoding('cl100k_base');
  }

  /**
   * Generar embedding con cach√© Redis
   */
  async generateEmbedding(text: string): Promise<number[]> {
    // 1. Validar input
    if (!text || text.trim().length === 0) {
      throw new Error('Text cannot be empty');
    }

    // Truncar si es muy largo (max 8191 tokens para text-embedding-3-large)
    const tokens = this.encoding.encode(text);
    if (tokens.length > 8000) {
      text = this.encoding.decode(tokens.slice(0, 8000));
    }

    // 2. Generar cache key (hash del texto)
    const textHash = crypto.createHash('sha256').update(text).digest('hex');
    const cacheKey = `embedding:${this.EMBEDDING_MODEL}:${this.EMBEDDING_DIMENSIONS}:${textHash}`;

    try {
      // 3. Buscar en cach√©
      const cached = await this.redis.get(cacheKey);
      if (cached) {
        console.log(`  ‚úÖ Embedding cache hit: ${textHash.substring(0, 8)}`);
        return JSON.parse(cached);
      }

      // 4. Generar embedding con cola (rate limiting)
      console.log(`  üîÑ Generating embedding: ${textHash.substring(0, 8)}`);
      const embedding = await this.generateWithQueue(text);

      // 5. Guardar en cach√©
      await this.redis.set(
        cacheKey,
        JSON.stringify(embedding),
        'EX',
        this.CACHE_TTL
      );

      return embedding;
    } catch (error: any) {
      console.error('‚ùå Error generating embedding:', error.message);
      throw new Error(`Failed to generate embedding: ${error.message}`);
    }
  }

  /**
   * Generar embedding usando cola para rate limiting
   */
  private generateWithQueue(text: string): Promise<number[]> {
    return new Promise((resolve, reject) => {
      this.queue.push({ text, resolve, reject });

      if (!this.processing) {
        this.processQueue();
      }
    });
  }

  /**
   * Procesar cola de embeddings con rate limiting
   */
  private async processQueue() {
    this.processing = true;

    try {
      while (this.queue.length > 0) {
        const batch = this.queue.splice(0, this.BATCH_SIZE);

        await Promise.all(
          batch.map(async (item) => {
            try {
              const embedding = await this.callOpenAIWithRetry(item.text);
              item.resolve(embedding);
            } catch (error: any) {
              item.reject(error);
            }
          })
        );

        // Delay entre batches
        if (this.queue.length > 0) {
          await this.delay(this.DELAY_MS);
        }
      }
    } catch (error: any) {
      console.error('‚ùå Queue processing error:', error.message);

      // Rechazar todos los items restantes en la cola
      while (this.queue.length > 0) {
        const item = this.queue.shift();
        if (item) {
          item.reject(new Error('Queue processing failed: ' + error.message));
        }
      }
    } finally {
      // Asegurar que el flag se resetea siempre
      this.processing = false;
    }
  }

  /**
   * Llamar a OpenAI con retry y exponential backoff
   */
  private async callOpenAIWithRetry(
    text: string,
    maxRetries = 5
  ): Promise<number[]> {
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        const response = await this.openai.embeddings.create({
          input: text,
          model: this.EMBEDDING_MODEL,
          dimensions: this.EMBEDDING_DIMENSIONS,
        });

        return response.data[0].embedding;
      } catch (error: any) {
        // Categorizar error
        if (error.status === 429) {
          // Rate limit - retry con backoff m√°s largo
          const backoff = Math.pow(2, attempt) * 1000; // 2s, 4s, 8s, 16s, 32s
          console.warn(`  ‚ö†Ô∏è  Rate limit hit, retrying in ${backoff}ms (attempt ${attempt}/${maxRetries})`);
          await this.delay(backoff);
          continue;
        }

        if (error.status >= 500) {
          // Server error - retry
          const backoff = Math.pow(2, attempt) * 500; // 1s, 2s, 4s, 8s, 16s
          console.warn(`  ‚ö†Ô∏è  Server error, retrying in ${backoff}ms (attempt ${attempt}/${maxRetries})`);
          await this.delay(backoff);
          continue;
        }

        if (error.status === 400) {
          // Bad request - no retry
          throw new Error(`Invalid request: ${error.message}`);
        }

        // √öltimo intento fallido
        if (attempt === maxRetries) {
          throw error;
        }
      }
    }

    throw new Error('Max retries exceeded');
  }

  /**
   * Chunking sem√°ntico inteligente
   */
  chunkText(
    text: string,
    metadata: Partial<ChunkMetadata> = {}
  ): DocumentChunk[] {
    if (!text || text.trim().length === 0) {
      return [];
    }

    // 1. Detectar tipo de contenido
    const contentType = this.detectContentType(text);

    // 2. Separar por l√≠mites sem√°nticos
    const semanticBlocks = this.splitSemanticBoundaries(text, contentType);

    // 3. Crear chunks con overlap
    const chunks: DocumentChunk[] = [];
    let previousText = '';

    for (const block of semanticBlocks) {
      const tokens = this.encoding.encode(block);

      if (tokens.length > this.CHUNK_SIZE) {
        // Bloque muy grande - dividir con cuidado
        const subChunks = this.splitLargeBlock(block, contentType);

        for (const subChunk of subChunks) {
          const fullText = previousText + subChunk;
          chunks.push({
            text: fullText,
            tokens: this.encoding.encode(fullText).length,
            metadata: { ...metadata, type: contentType } as ChunkMetadata,
            overlap: previousText,
          });

          // Overlap: √∫ltimas N tokens del chunk anterior
          previousText = this.getOverlap(subChunk);
        }
      } else if (tokens.length >= this.MIN_CHUNK_SIZE) {
        // Bloque de tama√±o adecuado
        const fullText = previousText + block;
        chunks.push({
          text: fullText,
          tokens: this.encoding.encode(fullText).length,
          metadata: { ...metadata, type: contentType } as ChunkMetadata,
          overlap: previousText,
        });

        previousText = this.getOverlap(block);
      }
      // Bloques muy peque√±os se acumulan en previousText
      else {
        previousText += block;
      }
    }

    return chunks;
  }

  /**
   * Detectar tipo de contenido
   */
  private detectContentType(text: string): 'paragraph' | 'table' | 'code' | 'list' {
    // Detectar tablas (markdown o pipes)
    if (text.match(/\|.*\|.*\|/g) || text.match(/\+[-+]+\+/g)) {
      return 'table';
    }

    // Detectar c√≥digo (bloques de c√≥digo o palabras clave de programaci√≥n)
    if (
      text.match(/```[\s\S]*```/) ||
      text.match(/function|class|const|let|var|def|import|export/g)
    ) {
      return 'code';
    }

    // Detectar listas
    if (text.match(/^[\s]*[-*\d]+[\.)]/gm)) {
      return 'list';
    }

    return 'paragraph';
  }

  /**
   * Separar por l√≠mites sem√°nticos
   */
  private splitSemanticBoundaries(
    text: string,
    type: 'paragraph' | 'table' | 'code' | 'list'
  ): string[] {
    switch (type) {
      case 'table':
        // NO separar tablas - mantenerlas intactas
        return [text];

      case 'code':
        // Separar por funciones/clases, no por l√≠neas
        return text.split(/(?=\n(?:function|class|const|export|def))/g);

      case 'list':
        // Agrupar √≠tems relacionados (separar por doble salto de l√≠nea)
        return text.split(/\n{2,}/g);

      default:
        // P√°rrafos: separar por puntos y seguidos, respetando may√∫sculas
        return text.split(/\.(?=\s+[A-Z√Å-√ö√ë])/g).map(s => s.trim() + '.');
    }
  }

  /**
   * Dividir bloque grande manteniendo contexto
   */
  private splitLargeBlock(
    block: string,
    type: 'paragraph' | 'table' | 'code' | 'list'
  ): string[] {
    // Si un bloque individual es muy grande, dividir por oraciones
    const sentences = block.split(/[.!?]\s+/);
    const chunks: string[] = [];
    let currentChunk = '';

    for (const sentence of sentences) {
      const testChunk = currentChunk + sentence + '. ';
      const tokens = this.encoding.encode(testChunk);

      if (tokens.length > this.CHUNK_SIZE) {
        if (currentChunk) chunks.push(currentChunk.trim());
        currentChunk = sentence + '. ';
      } else {
        currentChunk = testChunk;
      }
    }

    if (currentChunk) chunks.push(currentChunk.trim());

    return chunks.length > 0 ? chunks : [block]; // Fallback
  }

  /**
   * Obtener overlap del chunk anterior
   */
  private getOverlap(text: string): string {
    const tokens = this.encoding.encode(text);

    if (tokens.length <= this.OVERLAP_SIZE) {
      return text;
    }

    const overlapTokens = tokens.slice(-this.OVERLAP_SIZE);
    return this.encoding.decode(overlapTokens);
  }

  /**
   * Delay helper
   */
  private delay(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  /**
   * Cerrar conexiones
   * Note: Redis connection is managed by singleton, no need to close here
   */
  async close(): Promise<void> {
    // Redis connection managed by singleton - see utils/redis-singleton.ts
    console.log('‚úÖ Embedding service closed (Redis managed by singleton)');
  }
}

// Singleton instance
export const embeddingService = new EmbeddingService();
